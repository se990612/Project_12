# ğŸ“„ pages/2_ì¹´íƒˆë¡œê·¸_QnA.py
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_anthropic import ChatAnthropicMessages

# âœ… í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ğŸ“˜ ì¹´íƒˆë¡œê·¸ Q&A", layout="wide")
st.title("ğŸ“˜ ì°¨ëŸ‰ ì¹´íƒˆë¡œê·¸ ê¸°ë°˜ Claude RAG ì§ˆë¬¸ì‘ë‹µ")

ROOT_DIR = "C:/_knudata/hyundaicar_info"
VECTORSTORE_DIR = "C:/_knudata/vector_db/catalog"

if not os.getenv("ANTHROPIC_API_KEY"):
    st.error("âŒ .envì— ANTHROPIC_API_KEY ì„¤ì • í•„ìš”")
    st.stop()

# âœ… ì°¨ëŸ‰ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_all_car_models():
    models = []
    for category in os.listdir(ROOT_DIR):
        category_path = os.path.join(ROOT_DIR, category)
        if os.path.isdir(category_path):
            for model in os.listdir(category_path):
                model_path = os.path.join(category_path, model)
                if os.path.isdir(model_path):
                    models.append((os.path.join(category, model), model))
    return models

car_model_map = get_all_car_models()
car_model_options = [model_name for _, model_name in car_model_map]
selected_model = st.selectbox("ğŸš— ì°¨ëŸ‰ ì„ íƒ", car_model_options)

# âœ… FAISS ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_resource
def load_or_create_faiss(pdf_path, save_path):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    index_file = os.path.join(save_path, "index.faiss")
    pkl_file = os.path.join(save_path, "index.pkl")

    if os.path.exists(index_file) and os.path.exists(pkl_file):
        return FAISS.load_local(save_path, embeddings)
    else:
        docs = PyPDFLoader(pdf_path).load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(docs)

        vectordb = FAISS.from_documents(chunks, embeddings)
        if not os.path.exists(save_path):
            os.makedirs(save_path, exist_ok=True)
        vectordb.save_local(save_path)
        return vectordb

# âœ… Claude RAG ì‘ë‹µ í•¨ìˆ˜
def answer_with_claude(vectordb, query):
    retriever = vectordb.as_retriever()
    llm = ChatAnthropicMessages(model="claude-3-5-sonnet-20240620", temperature=0)
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
    return qa.run(query)

# âœ… PDF ê²½ë¡œ ë° ë²¡í„° ê²½ë¡œ
if selected_model:
    rel_path, model_name = next((fp, dn) for fp, dn in car_model_map if dn == selected_model)
    catalog_path = os.path.join(ROOT_DIR, rel_path, f"{model_name}-catalog.pdf")

    if not os.path.exists(catalog_path):
        st.error("âŒ í•´ë‹¹ ì°¨ëŸ‰ì˜ ì¹´íƒˆë¡œê·¸ PDFê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        st.success(f"ğŸ“„ {model_name} ì¹´íƒˆë¡œê·¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")
        question = st.text_input("â“ ì¹´íƒˆë¡œê·¸ ê¸°ë°˜ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

        if question:
            with st.spinner("Claude RAG ì‘ë‹µ ìƒì„± ì¤‘..."):
                faiss_dir = os.path.join(VECTORSTORE_DIR, model_name)
                vectordb = load_or_create_faiss(catalog_path, faiss_dir)
                response = answer_with_claude(vectordb, question)

            st.markdown("---")
            st.markdown(f"### ğŸš— ì°¨ëŸ‰: `{model_name}`")
            st.markdown(f"### â“ ì§ˆë¬¸: `{question}`")
            st.markdown("---")
            st.markdown("### ğŸ¤– Claude RAG ì‘ë‹µ")
            st.write(response)
