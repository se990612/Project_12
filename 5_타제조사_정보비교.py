import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS, Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain_anthropic import ChatAnthropic
import tempfile

# âœ… í™˜ê²½ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ğŸ“˜ í˜„ëŒ€ì°¨ vs íƒ€ì œì¡°ì‚¬ ë¹„êµ", layout="wide")
st.title("ğŸ“˜ Claude & GPT ê¸°ë°˜ í˜„ëŒ€ì°¨ vs íƒ€ì œì¡°ì‚¬ ì°¨ëŸ‰ ë¹„êµ ì§ˆì˜ì‘ë‹µ")

ROOT_DIR = "C:/_knudata/hyundaicar_info"
HYUNDAI_VECTOR_DIR = "C:/_knudata/vector_db/openai_catalog"

if not os.getenv("ANTHROPIC_API_KEY") or not os.getenv("OPENAI_API_KEY"):
    st.error("âŒ .envì— ANTHROPIC_API_KEY ë˜ëŠ” OPENAI_API_KEYê°€ ëˆ„ë½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    st.stop()

# âœ… í˜„ëŒ€ì°¨ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_all_car_models():
    models = []
    for category in os.listdir(ROOT_DIR):
        category_path = os.path.join(ROOT_DIR, category)
        if os.path.isdir(category_path):
            for model in os.listdir(category_path):
                model_path = os.path.join(category_path, model)
                if os.path.isdir(model_path):
                    models.append((model_path, model))
    return models

# âœ… í˜„ëŒ€ì°¨ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±/ë¡œë“œ
@st.cache_resource
def load_or_create_hyundai_vectorstore(catalog_path, price_path, save_dir):
    embeddings = OpenAIEmbeddings()
    index_file = os.path.join(save_dir, "index.faiss")
    pkl_file = os.path.join(save_dir, "index.pkl")

    if os.path.exists(index_file) and os.path.exists(pkl_file):
        return FAISS.load_local(save_dir, embeddings, allow_dangerous_deserialization=True)

    docs = PyPDFLoader(catalog_path).load() + PyPDFLoader(price_path).load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(docs)
    for doc in chunks:
        doc.metadata['source'] = 'hyundai'

    vectordb = FAISS.from_documents(chunks, embeddings)
    vectordb.save_local(save_dir)
    return vectordb

# âœ… íƒ€ì œì¡°ì‚¬ PDF â†’ ì„ì‹œ Chroma ìƒì„±
def create_temp_chroma_vectorstore(uploaded_pdf):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_pdf.read())
        temp_path = tmp_file.name

    docs = PyPDFLoader(temp_path).load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(docs)
    for doc in chunks:
        doc.metadata['source'] = 'other'

    tmp_dir = tempfile.mkdtemp()
    return Chroma.from_documents(chunks, embedding=OpenAIEmbeddings(), persist_directory=tmp_dir)

# âœ… QA ì²´ì¸
def build_qa_chain(model_name, retriever, provider="gpt"):
    if provider == "gpt":
        llm = ChatOpenAI(model_name=model_name, temperature=0)
    elif provider == "claude":
        llm = ChatAnthropic(model=model_name, temperature=0)
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì œê³µì")
    return RetrievalQAWithSourcesChain.from_chain_type(llm=llm, retriever=retriever)

# âœ… ë¬¸ì„œ ì¶”ì¶œ
def get_top_chunks(query, retriever, k=5):
    return retriever.get_relevant_documents(query)[:k]

# âœ… UI êµ¬ì„±
car_model_map = get_all_car_models()
car_model_options = [model_name for _, model_name in car_model_map]
selected_model = st.selectbox("ğŸš— í˜„ëŒ€ì°¨ ëª¨ë¸ ì„ íƒ", car_model_options)
uploaded_pdf = st.file_uploader("ğŸ“„ íƒ€ì œì¡°ì‚¬ ì°¨ëŸ‰ PDF ì—…ë¡œë“œ", type="pdf")
question = st.text_input("â“ ë¹„êµ ì§ˆë¬¸ ì…ë ¥")

if selected_model and uploaded_pdf and question:
    rel_path, model_name = next((fp, dn) for fp, dn in car_model_map if dn == selected_model)
    catalog_path = os.path.join(rel_path, f"{model_name}-catalog.pdf")
    price_path = os.path.join(rel_path, f"{model_name}-price.pdf")

    if not os.path.exists(catalog_path) or not os.path.exists(price_path):
        st.error("âŒ í˜„ëŒ€ì°¨ PDF ëˆ„ë½")
    else:
        with st.spinner("ğŸ“š ë²¡í„° ë¡œë”© ì¤‘"):
            vectordb_h = load_or_create_hyundai_vectorstore(catalog_path, price_path, os.path.join(HYUNDAI_VECTOR_DIR, model_name))
            vectordb_o = create_temp_chroma_vectorstore(uploaded_pdf)
            retriever_h = vectordb_h.as_retriever()
            retriever_o = vectordb_o.as_retriever()

        with st.spinner("ğŸ¤– Claude & GPT ì‘ë‹µ ìƒì„± ì¤‘"):
            # Direct LLM
            gpt_direct = ChatOpenAI(model_name="gpt-4o", temperature=0)
            gpt_direct_answer = gpt_direct.invoke(question).content

            claude_direct = ChatAnthropic(model="claude-3-haiku-20240307", temperature=0)
            claude_direct_answer = claude_direct.invoke(question).content

            # RAG
            gpt_chain_h = build_qa_chain("gpt-4o", retriever_h, "gpt")
            gpt_chain_o = build_qa_chain("gpt-4o", retriever_o, "gpt")
            gpt_h = gpt_chain_h.invoke({"question": question})
            gpt_o = gpt_chain_o.invoke({"question": question})

            claude_chain_h = build_qa_chain("claude-3-haiku-20240307", retriever_h, "claude")
            claude_chain_o = build_qa_chain("claude-3-haiku-20240307", retriever_o, "claude")
            claude_h = claude_chain_h.invoke({"question": question})
            claude_o = claude_chain_o.invoke({"question": question})

            top_chunks = get_top_chunks(question, retriever_h) + get_top_chunks(question, retriever_o)

        st.markdown(f"### âœ… ì§ˆë¬¸: {question}")
        st.subheader("ğŸ” ê¸°ë³¸ LLM ì‘ë‹µ (ì°¸ì¡° ì—†ìŒ)")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("ğŸ’¬ GPT-4o ì‘ë‹µ")
            st.info(gpt_direct_answer)
        with col2:
            st.markdown("ğŸ’¬ Claude ì‘ë‹µ")
            st.info(claude_direct_answer)

        st.markdown("### ğŸ“„ RAG ê¸°ë°˜ ë¹„êµ ì‘ë‹µ")
        col3, col4 = st.columns(2)
        with col3:
            st.markdown("ğŸ§  GPT - í˜„ëŒ€ì°¨")
            st.success(gpt_h["answer"])
            st.caption(f"ğŸ“š ì¶œì²˜: {gpt_h.get('sources', 'N/A')}")
            st.markdown("ğŸ§  GPT - íƒ€ì œì¡°ì‚¬")
            st.info(gpt_o["answer"])
        with col4:
            st.markdown("ğŸ§  Claude - í˜„ëŒ€ì°¨")
            st.success(claude_h["answer"])
            st.caption(f"ğŸ“š ì¶œì²˜: {claude_h.get('sources', 'N/A')}")
            st.markdown("ğŸ§  Claude - íƒ€ì œì¡°ì‚¬")
            st.info(claude_o["answer"])

        with st.expander("ğŸ“„ ì°¸ì¡° ë¬¸ì„œ ë³´ê¸°"):
            for i, doc in enumerate(top_chunks, 1):
                st.markdown(f"**ë¬¸ì„œ {i} ({doc.metadata.get('source', 'unknown')})**")
                st.code(doc.page_content.strip(), language="markdown")