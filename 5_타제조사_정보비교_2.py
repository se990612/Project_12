# ğŸ“„ pages/5_íƒ€ì œì¡°ì‚¬_ì •ë³´ë¹„êµ.py ìµœì¢…
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS, Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_anthropic import ChatAnthropic, ChatAnthropicMessages
from langchain.chains import RetrievalQAWithSourcesChain
import tempfile
from langchain.prompts import PromptTemplate

# âœ… í™˜ê²½ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ğŸš— í˜„ëŒ€ì°¨ vs íƒ€ì œì¡°ì‚¬ ë¹„êµ", layout="wide")
st.title("ğŸš— Claude & GPT ê¸°ë°˜ í˜„ëŒ€ì°¨ vs íƒ€ì œì¡°ì‚¬ ì°¨ëŸ‰ ë¹„êµ")

ROOT_DIR = "C:/_knudata/Project_12/hyundaicar_info"
VECTOR_DIR = "C:/_knudata/Project_12/vector_db/price"

# âœ… ì°¨ëŸ‰ ëª©ë¡
@st.cache_resource
def get_all_car_models():
    models = []
    for category in os.listdir(ROOT_DIR):
        category_path = os.path.join(ROOT_DIR, category)
        if not os.path.isdir(category_path):
            continue  # âœ… í´ë”ê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ (ex: .avif íŒŒì¼)

        for model in os.listdir(category_path):
            model_path = os.path.join(category_path, model)
            if os.path.isdir(model_path):  # âœ… ì—¬ê¸°ë„ ë””ë ‰í„°ë¦¬ í™•ì¸
                models.append((os.path.join(category, model), model))
    return models

car_model_map = get_all_car_models()
model_names = [name for _, name in car_model_map]

# âœ… UI
col1, col2 = st.columns(2)
with col1:
    selected_model = st.selectbox("âœ… ë¹„êµí•  í˜„ëŒ€ì°¨ ì„ íƒ", ["ì„ íƒ ì•ˆí•¨"] + model_names)
with col2:
    uploaded_pdf = st.file_uploader("ğŸ“¤ íƒ€ì œì¡°ì‚¬ ê°€ê²©í‘œ PDF ì—…ë¡œë“œ (ê¸°ì•„ ë“±)", type=["pdf"])

question = st.text_input("ğŸ’¬ ë¹„êµí•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë‘ ì°¨ëŸ‰ì˜ ê°€ê²© ì°¨ì´ëŠ” ì–¼ë§ˆì¸ê°€ìš”?)")

# âœ… í˜„ëŒ€ì°¨ FAISS
@st.cache_resource
def load_vectorstore(pdf_path, save_path):
    embeddings = OpenAIEmbeddings()
    if os.path.exists(os.path.join(save_path, "index.faiss")):
        return FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)
    else:
        docs = PyPDFLoader(pdf_path).load()
        chunks = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200).split_documents(docs)
        vectordb = FAISS.from_documents(chunks, embeddings)
        vectordb.save_local(save_path)
        return vectordb

# âœ… íƒ€ì œì¡°ì‚¬ Chroma
def create_temp_vectorstore(uploaded_file):
    if not uploaded_file: return None
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        path = tmp.name
    docs = PyPDFLoader(path).load()
    chunks = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200).split_documents(docs)
    return Chroma.from_documents(chunks, OpenAIEmbeddings())

# âœ… QA ì²´ì¸
def build_chain(model_name, retriever, provider):
    llm = ChatOpenAI(model_name=model_name) if provider == "gpt" else ChatAnthropic(model=model_name)

    prompt_template = PromptTemplate(
        input_variables=["summaries", "question"],
        template="""ë‹¹ì‹ ì€ ì°¨ëŸ‰ ê°€ê²©í‘œ PDFë¥¼ ë¶„ì„í•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë¬¸ì„œ ë‚´ìš©: {summaries}

í•œêµ­ì–´ë¡œ ë‹µë³€:
"""
    )

    return RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt_template}
    )

# âœ… Direct QA
def direct_answer(model_name, question, provider):
    llm = ChatOpenAI(model_name=model_name) if provider == "gpt" else ChatAnthropic(model=model_name)
    return llm.invoke(question).content

# âœ… ë¹„êµ ì‹¤í–‰
if selected_model != "ì„ íƒ ì•ˆí•¨" and uploaded_pdf and question:
    rel_path, model_name = next((p, n) for p, n in car_model_map if n == selected_model)
    hyundai_path = os.path.join(ROOT_DIR, rel_path, f"{model_name}-price.pdf")
    hyundai_vector = load_vectorstore(hyundai_path, os.path.join(VECTOR_DIR, model_name))
    competitor_vector = create_temp_vectorstore(uploaded_pdf)
    
    hyundai_retriever = hyundai_vector.as_retriever()
    competitor_retriever = competitor_vector.as_retriever()

    # âœ… Direct
    hyundai_gpt_direct = direct_answer("gpt-4o", question, "gpt")
    hyundai_claude_direct = direct_answer("claude-3-5-sonnet-20240620", question, "claude")
    competitor_gpt_direct = direct_answer("gpt-4o", question, "gpt")
    competitor_claude_direct = direct_answer("claude-3-5-sonnet-20240620", question, "claude")

    # âœ… RAG
    hyundai_gpt_rag = build_chain("gpt-4o", hyundai_retriever, "gpt").invoke({"question": question})
    hyundai_claude_rag = build_chain("claude-3-5-sonnet-20240620", hyundai_retriever, "claude").invoke({"question": question})
    competitor_gpt_rag = build_chain("gpt-4o", competitor_retriever, "gpt").invoke({"question": question})
    competitor_claude_rag = build_chain("claude-3-5-sonnet-20240620", competitor_retriever, "claude").invoke({"question": question})

    st.markdown(f"### â“ ì§ˆë¬¸: {question}")

    st.subheader("ğŸ” ê¸°ë³¸ LLM ì‘ë‹µ (PDF ë¯¸ì°¸ì¡°)")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"#### ğŸš— í˜„ëŒ€ì°¨ GPT-4o")
        st.info(hyundai_gpt_direct)
    with col2:
        st.markdown(f"#### ğŸ·ï¸ íƒ€ì œì¡°ì‚¬ GPT-4o")
        st.info(competitor_gpt_direct)
    col3, col4 = st.columns(2)
    with col3:
        st.markdown(f"#### ğŸš— í˜„ëŒ€ì°¨ Claude")
        st.info(hyundai_claude_direct)
    with col4:
        st.markdown(f"#### ğŸ·ï¸ íƒ€ì œì¡°ì‚¬ Claude")
        st.info(competitor_claude_direct)

    st.markdown("### ğŸ“„ RAG ê¸°ë°˜ ì‘ë‹µ (PDF ê¸°ë°˜)")
    col5, col6 = st.columns(2)
    with col5:
        st.markdown(f"#### ğŸš— í˜„ëŒ€ì°¨ GPT-4o + RAG")
        st.success(hyundai_gpt_rag["answer"])
        st.caption(f"ì¶œì²˜: {hyundai_gpt_rag.get('sources', 'N/A')}")
    with col6:
        st.markdown(f"#### ğŸ·ï¸ íƒ€ì œì¡°ì‚¬ GPT-4o + RAG")
        st.success(competitor_gpt_rag["answer"])
        st.caption(f"ì¶œì²˜: {competitor_gpt_rag.get('sources', 'N/A')}")

    col7, col8 = st.columns(2)
    with col7:
        st.markdown(f"#### ğŸš— í˜„ëŒ€ì°¨ Claude + RAG")
        st.success(hyundai_claude_rag["answer"])
        st.caption(f"ì¶œì²˜: {hyundai_claude_rag.get('sources', 'N/A')}")
    with col8:
        st.markdown(f"#### ğŸ·ï¸ íƒ€ì œì¡°ì‚¬ Claude + RAG")
        st.success(competitor_claude_rag["answer"])
        st.caption(f"ì¶œì²˜: {competitor_claude_rag.get('sources', 'N/A')}")


        # âœ… ìƒìœ„ ê´€ë ¨ ë¬¸ì„œ ë³´ê¸° í•¨ìˆ˜
    def get_top_chunks(query, retriever, k=5):
        return retriever.get_relevant_documents(query)[:k]

    # âœ… ì°¸ì¡° ë¬¸ì„œ ì¶œë ¥
    st.markdown("### ğŸ“š ì°¸ì¡°í•œ PDF ë¬¸ì„œ ë³´ê¸° (RAG ê¸°ë°˜)")
    with st.expander("ğŸš— í˜„ëŒ€ì°¨ ì°¸ì¡° ë¬¸ì„œ ë³´ê¸°"):
        hyundai_chunks = get_top_chunks(question, hyundai_retriever)
        for i, doc in enumerate(hyundai_chunks, 1):
            st.markdown(f"**ë¬¸ì„œ {i} ({doc.metadata.get('source', 'unknown')})**")
            st.code(doc.page_content.strip(), language="markdown")

    with st.expander("ğŸ·ï¸ íƒ€ì œì¡°ì‚¬ ì°¸ì¡° ë¬¸ì„œ ë³´ê¸°"):
        competitor_chunks = get_top_chunks(question, competitor_retriever)
        for i, doc in enumerate(competitor_chunks, 1):
            st.markdown(f"**ë¬¸ì„œ {i} ({doc.metadata.get('source', 'unknown')})**")
            st.code(doc.page_content.strip(), language="markdown")