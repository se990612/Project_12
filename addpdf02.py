# ğŸ“„ pages/5_íƒ€ì œì¡°ì‚¬_ì •ë³´ë¹„êµ.py
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS, Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_anthropic import ChatAnthropicMessages
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import tempfile

# âœ… í™˜ê²½ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ğŸš— í˜„ëŒ€ì°¨ vs íƒ€ì œì¡°ì‚¬ ë¹„êµ", layout="wide")
st.title("ğŸš— Claude ê¸°ë°˜ í˜„ëŒ€ì°¨ vs íƒ€ì œì¡°ì‚¬ ì°¨ëŸ‰ ë¹„êµ")

ROOT_DIR = "C:/gamin/Project_12_2/hyundaicar_info"
VECTOR_DIR = "C:/gamin/Project_12_2/vector_db/price"

# âœ… í˜„ëŒ€ì°¨ ëª¨ë¸ ëª©ë¡
def get_all_car_models():
    models = []
    for category in os.listdir(ROOT_DIR):
        category_path = os.path.join(ROOT_DIR, category)
        if os.path.isdir(category_path):
            for model in os.listdir(category_path):
                model_path = os.path.join(category_path, model)
                if os.path.isdir(model_path):
                    models.append((os.path.join(category, model), model))
    return models

car_model_map = get_all_car_models()
model_names = [name for _, name in car_model_map]

# âœ… UI êµ¬ì„±
col1, col2 = st.columns(2)

with col1:
    selected_model = st.selectbox("âœ… ë¹„êµí•  í˜„ëŒ€ì°¨ ì„ íƒ", ["ì„ íƒ ì•ˆí•¨"] + model_names)

with col2:
    uploaded_pdf = st.file_uploader("ğŸ“¤ íƒ€ì œì¡°ì‚¬ ê°€ê²©í‘œ PDF ì—…ë¡œë“œ (ê¸°ì•„ ë“±)", type=["pdf"])

question = st.text_input("ğŸ’¬ ë¹„êµí•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë‘ ì°¨ëŸ‰ì˜ ê°€ê²© ì°¨ì´ëŠ” ì–¼ë§ˆì¸ê°€ìš”?)")

# âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© í•¨ìˆ˜ (í˜„ëŒ€ì°¨ìš© FAISS)
@st.cache_resource
def load_hyundai_vectorstore(pdf_path, save_path):
    embeddings = OpenAIEmbeddings()
    if os.path.exists(os.path.join(save_path, "index.faiss")):
        return FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)
    else:
        docs = PyPDFLoader(pdf_path).load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        chunks = splitter.split_documents(docs)
        vectordb = FAISS.from_documents(chunks, embeddings)
        vectordb.save_local(save_path)
        return vectordb

# âœ… ì‹¤ì‹œê°„ PDF ë²¡í„° ì„ë² ë”© (Chroma ì‚¬ìš©)
def create_temp_vectorstore_from_pdf(uploaded_file):
    if uploaded_file is None:
        return None, []
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    docs = PyPDFLoader(tmp_path).load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
    chunks = splitter.split_documents(docs)
    embeddings = OpenAIEmbeddings()
    vectordb = Chroma.from_documents(chunks, embeddings)
    return vectordb, chunks

# âœ… Claude RAG ì§ˆì˜ì‘ë‹µ ì²´ì¸
def build_qa_chain(vectordb):
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    llm = ChatAnthropicMessages(model="claude-3-5-sonnet-20240620", temperature=0)
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
        ë„ˆëŠ” ì°¨ëŸ‰ ê°€ê²©í‘œ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ìë™ì°¨ ì „ë¬¸ê°€ì•¼.
        ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì¤˜.

        ì§ˆë¬¸: {question}
        ë¬¸ì„œ ì •ë³´:
        {context}
        ë‹µë³€:
        """
    )
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}
    )

# âœ… ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰
if selected_model != "ì„ íƒ ì•ˆí•¨" and uploaded_pdf and question:
    st.info("ğŸ” ì°¨ëŸ‰ ì •ë³´ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # í˜„ëŒ€ì°¨ vectordb
    rel_path, model_name = next((p, n) for p, n in car_model_map if n == selected_model)
    hyundai_pdf_path = os.path.join(ROOT_DIR, rel_path, f"{model_name}-price.pdf")
    hyundai_vectordb = load_hyundai_vectorstore(hyundai_pdf_path, os.path.join(VECTOR_DIR, model_name))
    hyundai_chain = build_qa_chain(hyundai_vectordb)

    # íƒ€ì œì¡°ì‚¬ vectordb + chunks
    competitor_vectordb, competitor_chunks = create_temp_vectorstore_from_pdf(uploaded_pdf)
    competitor_chain = build_qa_chain(competitor_vectordb)

    with st.spinner("ğŸ¤– Claudeê°€ í˜„ëŒ€ì°¨ ì‘ë‹µ ìƒì„± ì¤‘..."):
        hyundai_answer = hyundai_chain.run(question)
    with st.spinner("ğŸ¤– Claudeê°€ íƒ€ì œì¡°ì‚¬ ì‘ë‹µ ìƒì„± ì¤‘..."):
        competitor_answer = competitor_chain.run(question)

    # âœ… ê²°ê³¼ ì¶œë ¥
    st.markdown("### âœ… ë¹„êµ ê²°ê³¼")
    st.markdown(f"#### â“ ì§ˆë¬¸: `{question}`")
    st.markdown("---")
    st.markdown(f"#### ğŸš— í˜„ëŒ€ì°¨ `{model_name}` ì‘ë‹µ:")
    st.write(hyundai_answer)
    st.markdown("---")
    st.markdown("#### ğŸ·ï¸ íƒ€ì œì¡°ì‚¬ ì°¨ëŸ‰ ì‘ë‹µ:")
    st.write(competitor_answer)

    # âœ… ì°¸ì¡°í•œ PDF Chunk ë³´ê¸°
    with st.expander("ğŸ“„ ì°¸ì¡°í•œ ë¬¸ì„œ ë³´ê¸° (RAG ê¸°ë°˜)"):
        st.markdown("#### ë¬¸ì„œ 1 (price)")
        for i, chunk in enumerate(hyundai_vectordb.similarity_search(question, k=1), 1):
            st.code(chunk.page_content.strip(), language="markdown")

        st.markdown("#### ë¬¸ì„œ 2 (uploaded)")
        for i, chunk in enumerate(competitor_vectordb.similarity_search(question, k=1), 1):
            st.code(chunk.page_content.strip(), language="markdown")
