# ğŸ“¦ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.chains import RetrievalQAWithSourcesChain
from langchain_community.chat_models import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# âœ… .env ë¡œë“œ ë° í˜ì´ì§€ ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ğŸ“˜ Claude & GPT ì°¨ëŸ‰ ì§ˆì˜ì‘ë‹µ", layout="wide")
st.title("ğŸ“˜ Claude & GPT ê¸°ë°˜ ì°¨ëŸ‰ ì¹´íƒ€ë¡œê·¸ + ê°€ê²©í‘œ í†µí•© RAG ì§ˆë¬¸ì‘ë‹µ")

ROOT_DIR = "C:/gamin/Project_12_2/hyundaicar_info"
VECTORSTORE_DIR = "C:/gamin/Project_12_2/vector_db"

# âœ… API í‚¤ í™•ì¸
if not os.getenv("ANTHROPIC_API_KEY") or not os.getenv("OPENAI_API_KEY"):
    st.error("âŒ .envì— ANTHROPIC_API_KEY ë˜ëŠ” OPENAI_API_KEYê°€ ëˆ„ë½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    st.stop()

# âœ… ì°¨ëŸ‰ í´ë” êµ¬ì¡° íƒìƒ‰
def get_all_car_models():
    models = []
    for category in os.listdir(ROOT_DIR):
        category_path = os.path.join(ROOT_DIR, category)
        if os.path.isdir(category_path):
            for model in os.listdir(category_path):
                model_path = os.path.join(category_path, model)
                if os.path.isdir(model_path):
                    models.append((os.path.join(category, model), model))
    return models

# âœ… í†µí•© ë²¡í„°DB ìƒì„± or ë¶ˆëŸ¬ì˜¤ê¸° (OpenAI ì„ë² ë”© + ì°¸ì¡° í…Œê·µ í¬í•¨)
@st.cache_resource
def load_or_create_combined_vectorstore(catalog_path, price_path, save_path):
    embeddings = OpenAIEmbeddings()
    index_file = os.path.join(save_path, "index.faiss")
    pkl_file = os.path.join(save_path, "index.pkl")

    if os.path.exists(index_file) and os.path.exists(pkl_file):
        return FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    catalog_docs = PyPDFLoader(catalog_path).load()
    catalog_chunks = splitter.split_documents(catalog_docs)
    for doc in catalog_chunks:
        doc.metadata["source"] = "catalog"

    price_docs = PyPDFLoader(price_path).load()
    price_chunks = splitter.split_documents(price_docs)
    for doc in price_chunks:
        doc.metadata["source"] = "price"

    all_chunks = catalog_chunks + price_chunks
    vectordb = FAISS.from_documents(all_chunks, embeddings)
    os.makedirs(save_path, exist_ok=True)
    vectordb.save_local(save_path)
    return vectordb

# âœ… ì§ˆì˜ì‘ë‹µ ì²´ì¸ ìƒì„±
def build_qa_chain(model_name, retriever, provider="gpt"):
    if provider == "gpt":
        llm = ChatOpenAI(model_name=model_name, temperature=0)
    elif provider == "claude":
        llm = ChatAnthropic(model=model_name, temperature=0)
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì œê³µì")
    return RetrievalQAWithSourcesChain.from_chain_type(llm=llm, retriever=retriever)

# âœ… ìƒìœ„ ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
def get_top_chunks(query, retriever, k=5):
    return retriever.get_relevant_documents(query)[:k]

# âœ… Streamlit UI ì‹œì‘
car_model_map = get_all_car_models()
car_model_options = [model_name for _, model_name in car_model_map]
selected_model = st.selectbox("ğŸš— ì°¨ëŸ‰ ì„ íƒ", car_model_options)

if selected_model:
    rel_path, model_name = next((fp, dn) for fp, dn in car_model_map if dn == selected_model)
    catalog_path = os.path.join(ROOT_DIR, rel_path, f"{model_name}-catalog.pdf")
    price_path = os.path.join(ROOT_DIR, rel_path, f"{model_name}-price.pdf")

    if not os.path.exists(catalog_path):
        st.error("âŒ ì¹´íƒ€ë¡œê·¸ PDFê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    elif not os.path.exists(price_path):
        st.error("âŒ ê°€ê²©í‘œ PDFê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        st.success(f"ğŸ“„ `{model_name}`ì˜ ì¹´íƒ€ë¡œê·¸ + ê°€ê²©í‘œ ë¡œë“œ ì™„ë£Œ")

        question = st.text_input("â“ ì°¨ëŸ‰ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê°€ê²©, ì˜µì…˜, ì—°ë¹„ ë“±)")

        if question:
            with st.spinner("ğŸ“… ë²¡í„°ìŠ¤í†¤ ë¡œë“œ ì¤‘..."):
                vectordb_path = os.path.join(VECTORSTORE_DIR, model_name)
                vectordb = load_or_create_combined_vectorstore(catalog_path, price_path, vectordb_path)
                retriever = vectordb.as_retriever()

            with st.spinner("ğŸ¤– GPT & Claude ì‘ë‹µ ìƒì„± ì¤‘..."):
                gpt_chain = build_qa_chain("gpt-4o", retriever, provider="gpt")
                gpt_result = gpt_chain.invoke({"question": question})

                claude_chain = build_qa_chain("claude-3-haiku-20240307", retriever, provider="claude")
                claude_result = claude_chain.invoke({"question": question})

                top_chunks = get_top_chunks(question, retriever, k=5)

            # âœ… ê²°ê³¼ ì¶œë ¥
            st.markdown(f"### âœ… ì§ˆë¬¸: {question}")
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("ğŸ§  **GPT ì‘ë‹µ**")
                st.success(gpt_result["answer"])
                st.caption(f"ğŸ“š ì¶œì²˜: {gpt_result.get('sources', 'N/A')}")
            with col2:
                st.markdown("ğŸ§  **Claude ì‘ë‹µ**")
                st.success(claude_result["answer"])
                st.caption(f"ğŸ“š ì¶œì²˜: {claude_result.get('sources', 'N/A')}")

            with st.expander("ğŸ“„ ì°¸ì¡°í•œ ë¬¸ì„œ ë³´ê¸°"):
                for i, doc in enumerate(top_chunks, 1):
                    st.markdown(f"**ë¬¸ì„œ {i} ({doc.metadata.get('source', 'unknown')})**")
                    st.code(doc.page_content.strip(), language="markdown")
