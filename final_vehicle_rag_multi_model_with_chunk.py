# ğŸš˜ ì°¨ëŸ‰ í†µí•© ë¹„êµ RAG ì„œë¹„ìŠ¤ ìµœì¢…
import os
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
import pdfplumber

from langchain.docstore.document import Document
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_anthropic import ChatAnthropicMessages
from langchain_openai import ChatOpenAI

# âœ… ì´ˆê¸° ì„¤ì •
load_dotenv()
st.set_page_config(page_title="ğŸš˜ ì°¨ëŸ‰ ì •ë³´ ë¹„êµ RAG", layout="wide")
st.title("ğŸš˜ ì°¨ëŸ‰ ê°€ê²©/ì¹´íƒˆë¡œê·¸ ê¸°ë°˜ Claude & GPT RAG í†µí•© ì‘ë‹µ")

ROOT_DIR = "C:/Users/KDT13/kh0616/project_12/Project_12/hyundaicar_info"
VECTOR_DIR = "C:/Users/KDT13/kh0616/project_12/Project_12/vector_db"
CATALOG_DB = os.path.join(VECTOR_DIR, "catalog")
PRICE_DB = os.path.join(VECTOR_DIR, "price")

if not os.getenv("ANTHROPIC_API_KEY") or not os.getenv("OPENAI_API_KEY"):
    st.error("âŒ .envì— ANTHROPIC_API_KEY ë˜ëŠ” OPENAI_API_KEY ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()

# âœ… ì°¨ëŸ‰ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_resource
def get_all_car_models():
    models = []
    for category in os.listdir(ROOT_DIR):
        category_path = os.path.join(ROOT_DIR, category)
        if not os.path.isdir(category_path):
            continue  # ì´ë¯¸ì§€ ë“± íŒŒì¼ì€ ë¬´ì‹œ

        for model_dir in os.listdir(category_path):
            model_path = os.path.join(category_path, model_dir)
            if not os.path.isdir(model_path):
                continue  # í•˜ìœ„ í´ë”ë„ ë””ë ‰í„°ë¦¬ë§Œ

            catalog_file = next((f for f in os.listdir(model_path) if f.endswith("-catalog.pdf")), None)
            price_file = next((f for f in os.listdir(model_path) if f.endswith("-price.pdf")), None)
            if catalog_file and price_file:
                base_name = catalog_file.replace("-catalog.pdf", "")
                rel_path = os.path.relpath(model_path, ROOT_DIR)
                models.append((model_dir, rel_path, base_name))
    return models


# âœ… PDF + í‘œ ë³‘í•© ì»¤ìŠ¤í…€ ë¡œë”
def custom_loader_with_table(pdf_path):
    docs = PyMuPDFLoader(pdf_path).load()
    table_texts = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            tables = page.extract_tables()
            for table in tables:
                if not table or not table[0]: continue
                header = table[0]
                for row in table[1:]:
                    if row and any(cell for cell in row):
                        clean_row = [cell.strip() if cell else "" for cell in row]
                        row_text = ", ".join(f"{k.strip()}: {v.strip()}" for k, v in zip(header, clean_row))
                        table_texts.append(row_text)
    if table_texts:
        docs.append(Document(page_content="\n".join(table_texts)))
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)
    return splitter.split_documents(docs)

# âœ… ë²¡í„° DB ë¡œë“œ/ìƒì„±
@st.cache_resource
def load_or_create_faiss(pdf_path, save_path):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    index_file = os.path.join(save_path, "index.faiss")
    pkl_file = os.path.join(save_path, "index.pkl")
    if os.path.exists(index_file) and os.path.exists(pkl_file):
        return FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)
    else:
        docs = custom_loader_with_table(pdf_path)
        vectordb = FAISS.from_documents(docs, embeddings)
        os.makedirs(save_path, exist_ok=True)
        vectordb.save_local(save_path)
        return vectordb

# âœ… ì˜ˆì‚° í•„í„°
st.markdown("### ğŸ’¸ ë¨¼ì €, ì˜ˆì‚°ì„ ì„¤ì •í•´ì£¼ì„¸ìš”!")
budget_range = st.slider("ì˜ˆì‚° ë²”ìœ„ (ë§Œì›)", 1000, 7000, (2500, 5000), step=100)

benefit_data = [
    ("ê·¸ëœì €", 3798, 170), ("ê·¸ëœì € Hybrid", 4354, 170), ("ì•„ë°˜ë–¼", 2034, 155), ("ì•„ë°˜ë–¼ Hybrid", 2523, 155),
    ("ì˜ë‚˜íƒ€ ë”” ì—£ì§€", 2788, 250), ("ì˜ë‚˜íƒ€ ë”” ì—£ì§€ Hybrid", 3232, 250), ("ì½”ë‚˜", 2446, 155), ("ì½”ë‚˜ Hybrid", 2955, 155),
    ("ë² ë‰´", 1956, 155), ("ë”” ì˜¬ ë‰´ íŒ°ë¦¬ì„¸ì´ë“œ", 4383, 126), ("ë”” ì˜¬ ë‰´ íŒ°ë¦¬ì„¸ì´ë“œ Hybrid", 4968, 126),
    ("íˆ¬ì‹¼", 2729, 250), ("íˆ¬ì‹¼ Hybrid", 3205, 250), ("ì‹¼íƒ€í˜", 3492, 250), ("ì‹¼íƒ€í˜ Hybrid", 3870, 150),
    ("ìŠ¤íƒ€ë¦¬ì•„ ë¼ìš´ì§€", 3780, 335), ("ìŠ¤íƒ€ë¦¬ì•„ ë¼ìš´ì§€ Hybrid", 4110, 235), ("ìŠ¤íƒ€ë¦¬ì•„", 2847, 335),
    ("ìŠ¤íƒ€ë¦¬ì•„ Hybrid", 3433, 235), ("ìŠ¤íƒ€ë¦¬ì•„ í‚¨ë”", 3643, 335), ("ìŠ¤íƒ€ë¦¬ì•„ ë¼ìš´ì§€ ìº í¼", 7094, 335),
    ("ìŠ¤íƒ€ë¦¬ì•„ ë¼ìš´ì§€ ìº í¼ Hybrid", 7436, 235), ("ìŠ¤íƒ€ë¦¬ì•„ ë¼ìš´ì§€ ë¦¬ë¬´ì§„", 5911, 335),
    ("ìŠ¤íƒ€ë¦¬ì•„ ë¼ìš´ì§€ ë¦¬ë¬´ì§„ Hybrid", 6241, 235), ("ë” ë‰´ ì•„ì´ì˜¤ë‹‰ 6", 4856, 780), ("ë”” ì˜¬ ë‰´ ë„¥ì˜", 7643, 495),
    ("ì•„ì´ì˜¤ë‹‰ 5", 4740, 600), ("ì½”ë‚˜ Electric", 4152, 685), ("ì•„ì´ì˜¤ë‹‰ 9", 6715, 370), ("ST1", 5655, 475),
    ("í¬í„° II Electric", 4325, 485), ("ì•„ë°˜ë–¼ N", 3360, 455), ("ì•„ì´ì˜¤ë‹‰ 5 N", 7700, 780), ("í¬í„° II", 2028, 185)
]

filtered = []
for name, start_price, max_discount in benefit_data:
    discount_price = start_price - max_discount
    if budget_range[0] <= start_price <= budget_range[1] or budget_range[0] <= discount_price <= budget_range[1]:
        filtered.append((name, start_price, max_discount, discount_price))

if filtered:
    df_filtered = pd.DataFrame(filtered, columns=["ì°¨ëŸ‰ëª…", "ì‹œì‘ê°€ (ë§Œì›)", "ìµœëŒ€ í• ì¸ (ë§Œì›)", "í˜œíƒ ì ìš©ê°€ (ë§Œì›)"])
    st.markdown("### ğŸ“Š ì •ë ¬ ì˜µì…˜ ì„ íƒ")
    sort_column = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["í˜œíƒ ì ìš©ê°€ (ë§Œì›)", "ì‹œì‘ê°€ (ë§Œì›)", "ìµœëŒ€ í• ì¸ (ë§Œì›)"])
    sort_order = st.radio("ì •ë ¬ ë°©ì‹", ["ì˜¤ë¦„ì°¨ìˆœ", "ë‚´ë¦¼ì°¨ìˆœ"], horizontal=True)
    ascending = True if sort_order == "ì˜¤ë¦„ì°¨ìˆœ" else False
    df_filtered = df_filtered.sort_values(sort_column, ascending=ascending).reset_index(drop=True)
    st.success(f"ğŸ” ì˜ˆì‚° {budget_range[0]}~{budget_range[1]}ë§Œì› ì°¨ëŸ‰ {len(df_filtered)}ê°œ")
    st.dataframe(df_filtered, use_container_width=True)
else:
    st.warning("í˜„ì¬ ì˜ˆì‚°ì— ë§ëŠ” ì°¨ëŸ‰ì´ ì—†ìŠµë‹ˆë‹¤.")

# âœ… ì°¨ëŸ‰ ì„ íƒ
st.markdown("### ğŸš— ì°¨ëŸ‰ ì„ íƒ")
car_model_map = get_all_car_models()
car_model_options = [display_name for display_name, _, _ in car_model_map]
selected_model = st.selectbox("ì°¨ëŸ‰ì„ ì„ íƒí•˜ì„¸ìš”", ["ì„ íƒ ì•ˆí•¨"] + car_model_options)

if selected_model != "ì„ íƒ ì•ˆí•¨":
    result = next(((rel_path, base_name) for display_name, rel_path, base_name in car_model_map if display_name == selected_model), None)
    if result:
        rel_path, base_name = result
        st.success(f"âœ… `{selected_model}` ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    else:
        st.error("âŒ ì°¨ëŸ‰ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
else:
    st.stop()

# âœ… PDF ê²½ë¡œ ê²°ì • í•¨ìˆ˜
def find_relevant_pdf(question, model_name, rel_path):
    price_keywords = ["ê°€ê²©", "í• ì¸", "ë¹„ìš©", "ê¸ˆì•¡", "ì¶œê³ ê°€", "ì˜µì…˜ê°€"]
    pdf_type = "price" if any(kw in question for kw in price_keywords) else "catalog"
    filename = f"{model_name}-{pdf_type}.pdf"
    save_path = os.path.join(PRICE_DB if pdf_type == "price" else CATALOG_DB, model_name)
    return os.path.join(ROOT_DIR, rel_path, filename), save_path

# âœ… RAG ì‘ë‹µ í•¨ìˆ˜
def get_rag_answer(llm, vectordb, query, name=""):
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template=f"""
        ì°¨ëŸ‰ ì¹´íƒˆë¡œê·¸/ê°€ê²©í‘œ PDFë¥¼ ë¶„ì„í•´ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
        ì§ˆë¬¸: {{question}}
        ë¬¸ì„œ ì •ë³´: {{context}}
        ë‹µë³€:
        """
    )
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type_kwargs={"prompt": prompt_template})
    return qa.run(query)

# âœ… ì¼ë°˜ ì‘ë‹µ í•¨ìˆ˜
def get_basic_answer(llm, question):
    return llm.invoke(question).content

# âœ… ì°¸ì¡° chunk ë¯¸ë¦¬ë³´ê¸° í•¨ìˆ˜
def get_context_chunks(vectordb, query):
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    docs = retriever.get_relevant_documents(query)
    return docs

# ğŸ” ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "chat_log" not in st.session_state:
    st.session_state["chat_log"] = []

# âœ… ì§ˆë¬¸ ì…ë ¥
question = st.text_input("ğŸ’¬ ì°¨ëŸ‰ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if question and selected_model != "ì„ íƒ ì•ˆí•¨":
    with st.spinner("â³ ë²¡í„° DB ë¡œë“œ ì¤‘..."):
        pdf_path, vector_path = find_relevant_pdf(question, base_name, rel_path)
        vectordb = load_or_create_faiss(pdf_path, vector_path)

    with st.spinner("ğŸ§  Claude & GPT ì‘ë‹µ ìƒì„± ì¤‘..."):
        # Claude vs Claude+RAG
        claude_llm = ChatAnthropicMessages(model="claude-3-5-sonnet-20240620", temperature=0.3)
        claude_rag = get_rag_answer(claude_llm, vectordb, question, "claude_rag")
        claude_basic = get_basic_answer(claude_llm, question)

        # GPT vs GPT+RAG
        gpt_llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
        gpt_rag = get_rag_answer(gpt_llm, vectordb, question, "gpt_rag")
        gpt_basic = get_basic_answer(gpt_llm, question)

        # ì°¸ì¡° chunk
        context_chunks = get_context_chunks(vectordb, question)

    # ğŸ” íˆìŠ¤í† ë¦¬ ì €ì¥
    st.session_state["chat_log"].append({
        "question": question,
        "claude_rag": claude_rag,
        "claude_basic": claude_basic,
        "gpt_rag": gpt_rag,
        "gpt_basic": gpt_basic,
        "chunks": context_chunks
    })

# âœ… ì „ì²´ ëŒ€í™” ê¸°ë¡ ë³´ê¸°
for i, log in enumerate(reversed(st.session_state["chat_log"])):
    st.markdown(f"### ğŸ” ì§ˆë¬¸ {len(st.session_state['chat_log']) - i}: `{log['question']}`")

    with st.expander("ğŸ¤– Claude (ê¸°ë³¸ vs RAG)", expanded=True):
        st.markdown("**Claude (ê¸°ë³¸):**")
        st.markdown(log["claude_basic"])
        st.markdown("**Claude + RAG:**")
        st.markdown(log["claude_rag"])

    with st.expander("ğŸ§  GPT (ê¸°ë³¸ vs RAG)", expanded=False):
        st.markdown("**GPT (ê¸°ë³¸):**")
        st.markdown(log["gpt_basic"])
        st.markdown("**GPT + RAG:**")
        st.markdown(log["gpt_rag"])

    with st.expander("ğŸ“„ ì°¸ì¡°í•œ PDF ë¬¸ì„œ Chunk", expanded=False):
        for idx, doc in enumerate(log["chunks"]):
            st.markdown(f"**Chunk {idx+1}:**")
            st.code(doc.page_content[:1000], language="text")  # ë„ˆë¬´ ê¸¸ ê²½ìš° ì•ë¶€ë¶„ë§Œ í‘œì‹œ
