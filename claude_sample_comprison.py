import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_anthropic import ChatAnthropicMessages

# ğŸ” API KEY ë¡œë“œ
load_dotenv()
if not os.getenv("ANTHROPIC_API_KEY"):
    st.error("âŒ .env íŒŒì¼ì— ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# ğŸ“ í˜„ëŒ€ì°¨ PDF ê²½ë¡œ
ROOT_DIR = "C:/_knudata/hyundaicar_info"
if not os.path.exists(ROOT_DIR):
    st.error(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ROOT_DIR}")
    st.stop()

# ğŸ” ì „ì²´ ì°¨ëŸ‰ ë¦¬ìŠ¤íŠ¸
def get_all_car_models():
    models = []
    for category in os.listdir(ROOT_DIR):
        category_path = os.path.join(ROOT_DIR, category)
        if os.path.isdir(category_path):
            for model in os.listdir(category_path):
                model_path = os.path.join(category_path, model)
                if os.path.isdir(model_path):
                    models.append((os.path.join(category, model), model))
    return models

car_model_map = get_all_car_models()
car_model_options = [model_name for _, model_name in car_model_map]
selected_models = st.multiselect("ğŸš˜ ì°¨ì¢… ì„ íƒ (1ê°œ ë˜ëŠ” 2ê°œ)", car_model_options)

# ğŸ”„ PDF â†’ í•˜ë‚˜ì˜ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def load_vectorstore_combined(pdf_paths):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    all_docs = []
    for path in pdf_paths:
        docs = PyPDFLoader(path).load()
        chunks = splitter.split_documents(docs)
        all_docs.extend(chunks)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = Chroma.from_documents(all_docs, embedding=embeddings)
    return vectordb

# ğŸ¤– Claude ì‘ë‹µ
def answer_with_claude(vectorstore, query):
    retriever = vectorstore.as_retriever()
    llm = ChatAnthropicMessages(
        model="claude-3-5-sonnet-20240620",
        temperature=0,
        api_key=os.getenv("ANTHROPIC_API_KEY")
    )
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
    return qa_chain.run(query)

# âœ… ì°¨ëŸ‰ 1ê°œ: ê¸°ë³¸ ì‘ë‹µ + Hyundai RAG ì‘ë‹µ
if len(selected_models) == 1:
    model_rel_path, model_name = next((fp, dn) for fp, dn in car_model_map if dn == selected_models[0])
    base_path = os.path.join(ROOT_DIR, model_rel_path)
    catalog_path = os.path.join(base_path, f"{model_name}-catalog.pdf")
    price_path = os.path.join(base_path, f"{model_name}-price.pdf")

    if not os.path.exists(catalog_path) or not os.path.exists(price_path):
        st.error("âŒ ì¹´íƒˆë¡œê·¸ ë˜ëŠ” ê°€ê²©í‘œ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        vectordb = load_vectorstore_combined([catalog_path, price_path])

        st.subheader(f"ğŸ“˜ [{model_name}] ì°¨ëŸ‰ ì§ˆë¬¸")
        question = st.text_input("â“ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”")

        if question:
            with st.spinner("Claude ì‘ë‹µ ìƒì„± ì¤‘..."):
                llm = ChatAnthropicMessages(
                    model="claude-3-5-sonnet-20240620",
                    temperature=0,
                    api_key=os.getenv("ANTHROPIC_API_KEY")
                )

                # 1ï¸âƒ£ Claude ê¸°ë³¸ ì‘ë‹µ
                base_response = llm.invoke(question).content

                # 2ï¸âƒ£ Hyundai RAG
                rag_qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb.as_retriever())
                rag_response = rag_qa.invoke({"query": question})["result"]

            # ğŸ” ì‘ë‹µ ë¹„êµ ì¶œë ¥
            st.markdown("---")
            st.markdown(f"### ğŸš— ì°¨ëŸ‰: `{model_name}` | â“ ì§ˆë¬¸: `{question}`")

            col1, col2 = st.columns(2)
            with col1:
                st.markdown("ğŸ§  **Claude ê¸°ë³¸ ì‘ë‹µ**")
                st.write(base_response)
            with col2:
                st.markdown("ğŸ” **í˜„ëŒ€ì°¨ PDF ê¸°ë°˜ RAG ì‘ë‹µ**")
                st.write(rag_response)

# âœ… ì°¨ëŸ‰ 2ê°œ ì„ íƒ: Hyundai RAG ì‘ë‹µ ë¹„êµ
elif len(selected_models) == 2:
    st.subheader(f"ğŸ“Š ì°¨ëŸ‰ ë¹„êµ: {selected_models[0]} vs {selected_models[1]}")
    question = st.text_input("ë¹„êµí•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    def load_both_vectorstores():
        vectordbs = []
        for name in selected_models:
            rel_path, model_name = next((fp, dn) for fp, dn in car_model_map if dn == name)
            base_path = os.path.join(ROOT_DIR, rel_path)
            catalog_path = os.path.join(base_path, f"{model_name}-catalog.pdf")
            price_path = os.path.join(base_path, f"{model_name}-price.pdf")
            if os.path.exists(catalog_path) and os.path.exists(price_path):
                vectordb = load_vectorstore_combined([catalog_path, price_path])
                vectordbs.append((model_name, vectordb))
        return vectordbs

    if question:
        db_pairs = load_both_vectorstores()
        if len(db_pairs) != 2:
            st.error("â— ë‘ ì°¨ëŸ‰ì˜ ì¹´íƒˆë¡œê·¸/ê°€ê²©í‘œ PDFê°€ ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("Claude RAG ë¹„êµ ì‘ë‹µ ìƒì„± ì¤‘..."):
                rag1 = answer_with_claude(db_pairs[0][1], question)
                rag2 = answer_with_claude(db_pairs[1][1], question)

            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"### ğŸ“˜ {db_pairs[0][0]} (í˜„ëŒ€ì°¨ RAG ì‘ë‹µ)")
                st.write(rag1)
            with col2:
                st.markdown(f"### ğŸ“˜ {db_pairs[1][0]} (í˜„ëŒ€ì°¨ RAG ì‘ë‹µ)")
                st.write(rag2)
