# ğŸ” API Key í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
import os
from dotenv import load_dotenv
load_dotenv()
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

# ğŸ“¦ ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# ğŸ“ PDF ë£¨íŠ¸ ê²½ë¡œ
PDF_ROOT = "C:/_knudata/hyundaicar_info"

# ğŸ“˜ Streamlit ì„¤ì •
st.set_page_config(page_title="í˜„ëŒ€ì°¨ RAG ë°ëª¨", layout="wide")
st.title("ğŸš— í˜„ëŒ€ìë™ì°¨ GPT vs PDF RAG ë¹„êµ ì„œë¹„ìŠ¤")
st.caption("ì°¨ëŸ‰ ì¹´íƒˆë¡œê·¸ ë° ê°€ê²©í‘œ PDF ê¸°ë°˜ ì‘ë‹µê³¼ GPT ê¸°ë³¸ ì‘ë‹µ ë¹„êµ")

# ğŸ“‚ ì°¨ì¢… ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (í•˜ìœ„ í´ë” + PDF í¬í•¨ í´ë” ëª¨ë‘)
def get_all_models():
    model_list = []
    for category in os.listdir(PDF_ROOT):
        category_path = os.path.join(PDF_ROOT, category)
        if not os.path.isdir(category_path):
            continue

        has_pdf = any(f.endswith(".pdf") for f in os.listdir(category_path))
        if has_pdf:
            model_list.append((category, None))  # ex. ("ìŠ¹ìš©", None)

        for model in os.listdir(category_path):
            model_path = os.path.join(category_path, model)
            if os.path.isdir(model_path):
                model_list.append((category, model))  # ex. ("ìŠ¹ìš©", "ê·¸ëœì €")

    return model_list

# ğŸš˜ ëª¨ë¸ ì„ íƒ UI
model_options = get_all_models()
selected_cat_model = st.selectbox("ğŸš˜ ì°¨ì¢… ì„ íƒ", model_options, format_func=lambda x: f"{x[0]}" if x[1] is None else f"{x[0]} - {x[1]}")
selected_category, selected_model = selected_cat_model

# ğŸ“„ ì‹¤ì œ PDF í´ë” ê²½ë¡œ ì„¤ì •
if selected_model is None:
    model_dir = os.path.join(PDF_ROOT, selected_category)
    selected_model_name = selected_category
else:
    model_dir = os.path.join(PDF_ROOT, selected_category, selected_model)
    selected_model_name = selected_model

# ğŸ“„ PDF ê²½ë¡œ íŒŒì‹±
catalog_path, price_path = None, None
for file in os.listdir(model_dir):
    if "catalog" in file.lower():
        catalog_path = os.path.join(model_dir, file)
    elif "price" in file.lower():
        price_path = os.path.join(model_dir, file)

# ğŸ§± ê²½ë¡œ ì ê²€
if not catalog_path or not price_path:
    st.error("ì¹´íƒˆë¡œê·¸ ë˜ëŠ” ê°€ê²©í‘œ PDF íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.stop()

st.write(f"ğŸ“˜ ì¹´íƒˆë¡œê·¸ íŒŒì¼: `{os.path.basename(catalog_path)}`")
st.write(f"ğŸ’° ê°€ê²©í‘œ íŒŒì¼: `{os.path.basename(price_path)}`")

# ğŸ“¦ ë²¡í„°ìŠ¤í† ì–´ ìºì‹œ
@st.cache_resource
def load_vectorstores(catalog_path, price_path, car_id):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    embedding = OpenAIEmbeddings()

    catalog_chunks = splitter.split_documents(PyPDFLoader(catalog_path).load())
    price_chunks = splitter.split_documents(PyPDFLoader(price_path).load())

    catalog_db = Chroma.from_documents(catalog_chunks, embedding=embedding, collection_name=f"{car_id}_catalog")
    price_db = Chroma.from_documents(price_chunks, embedding=embedding, collection_name=f"{car_id}_price")

    return catalog_db, price_db

catalog_db, price_db = load_vectorstores(catalog_path, price_path, selected_model_name)

# âœ… ì§ˆë¬¸ ì…ë ¥
question = st.text_input("â“ ì°¨ëŸ‰ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”", placeholder="ì˜ˆ: í•˜ì´ë¸Œë¦¬ë“œ ì—°ë¹„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")

# âœ… ì‘ë‹µ ìƒì„±
if question:
    with st.spinner("ğŸ¤– ì‘ë‹µ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        llm = ChatOpenAI(model="gpt-4o")

        # 1ï¸âƒ£ GPT ê¸°ë³¸ ì‘ë‹µ
        gpt_response = llm.invoke(question).content

        # 2ï¸âƒ£ ì¹´íƒˆë¡œê·¸ ê¸°ë°˜ ì‘ë‹µ
        catalog_qa = RetrievalQA.from_chain_type(llm=llm, retriever=catalog_db.as_retriever())
        catalog_response = catalog_qa.invoke({"query": question})["result"]

        # 3ï¸âƒ£ ê°€ê²©í‘œ ê¸°ë°˜ ì‘ë‹µ
        price_qa = RetrievalQA.from_chain_type(llm=llm, retriever=price_db.as_retriever())
        price_response = price_qa.invoke({"query": question})["result"]

    # ğŸ§¾ ì¶œë ¥
    st.markdown("---")
    st.markdown(f"### ğŸš— ì°¨ëŸ‰: `{selected_model_name}` | â“ ì§ˆë¬¸: `{question}`")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("ğŸ§  **GPT ê¸°ë³¸ ì‘ë‹µ**")
        st.write(gpt_response)
    with col2:
        st.markdown("ğŸ“˜ **ì¹´íƒˆë¡œê·¸ ê¸°ë°˜ ì‘ë‹µ**")
        st.write(catalog_response)
    with col3:
        st.markdown("ğŸ’° **ê°€ê²©í‘œ ê¸°ë°˜ ì‘ë‹µ**")
        st.write(price_response)
